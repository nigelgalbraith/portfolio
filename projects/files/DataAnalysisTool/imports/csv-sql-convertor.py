#!/usr/bin/env python3
"""
CSV-to-SQL import generator (PostgreSQL).

What this does (per job in config):
1) Reads a CSV where **each row corresponds to one record** in the entity table.
2) Creates three table types:
   - lookup tables: id + unique name
   - entity table: one row per CSV row
   - junction tables: many-to-many links based on CSV multi-value columns
3) Emits SQL in the correct order:
   - CREATE TABLEs
   - (optional) seed UI metadata table
   - INSERT lookup values (distinct, ON CONFLICT DO NOTHING)
   - INSERT entity rows (optionally using CSV "ID" for deterministic ids)
   - INSERT junction rows (ON CONFLICT DO NOTHING)

It also prints a clear summary at the end of each job so you can see what was created/inserted
without having to manually inspect the database.

Config:
    imports/config/import_config.json

Output:
    per-job SQL file, e.g. imports/sql/job_tracker.sql
"""

import csv
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Iterable, Set
from datetime import datetime

CONFIG_PATH = "config/import_config.json"

RESERVED = {
    "user", "group", "order", "select", "where", "from", "table",
    "references", "constraint", "primary", "foreign", "join", "limit",
    "create", "drop", "insert", "update", "delete"
}

# -------------------- NORMALIZATION --------------------

def normalize(name: str) -> str:
    """Normalize a name to safe snake_case and avoid reserved words."""
    name = (name or "").lower().strip()
    name = re.sub(r"[^\w]+", "_", name)
    name = re.sub(r"_+", "_", name).strip("_")
    if not name:
        name = "unnamed"
    if name[0].isdigit():
        name = f"t_{name}"
    if name in RESERVED:
        name = f"{name}_col"
    return name

def q(name: str) -> str:
    """Quote an SQL identifier using double-quotes."""
    return f'"{name}"'

def sql_quote_text(v: str) -> str:
    """Quote and escape a SQL text literal using single-quotes."""
    return "'" + (v or "").replace("'", "''") + "'"

# -------------------- DATE --------------------

def parse_date_to_iso(v: str, fmts: List[str]) -> Optional[str]:
    """Parse a date string using a list of accepted formats. Returns ISO date or None."""
    s = (v or "").strip()
    if not s:
        return None
    for fmt in fmts:
        try:
            return datetime.strptime(s, fmt).date().isoformat()
        except ValueError:
            continue
    return None

# -------------------- CSV VALUE HELPERS --------------------

def split_values(raw: str, split_spec: str) -> List[str]:
    """Split a cell into multiple values using config split spec (supports \\n)."""
    if raw is None:
        return []
    s = str(raw).strip()
    if not s:
        return []
    sep = (split_spec or "\\n").encode("utf-8").decode("unicode_escape")  # "\\n" -> "\n"
    parts = [p.strip() for p in s.split(sep)]
    return [p for p in parts if p]

def get_csv_id(row: Dict[str, str]) -> Optional[int]:
    """Return integer ID from a CSV row if an ID-like column exists."""
    for key in ("ID", "Id", "id"):
        if key in row and str(row[key]).strip():
            try:
                return int(str(row[key]).strip())
            except ValueError:
                return None
    return None

# -------------------- SQL BUILDERS --------------------

def build_lookup_table(schema: str, name: str, pk: str) -> str:
    """Build CREATE TABLE SQL for a lookup table (identity PK + unique name)."""
    return f"""
CREATE TABLE {q(schema)}.{q(name)} (
  {q(pk)} BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  name TEXT UNIQUE
);
""".strip()

def build_entity_table(schema: str, name: str, pk: str, fields: List[Dict[str, Any]]) -> str:
    """Build CREATE TABLE SQL for an entity table (identity PK + typed columns + FK constraints)."""
    cols: List[str] = [
        f'  {q(pk)} BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY'
    ]
    fk_constraints: List[str] = []
    for f in fields:
        col = f["col"]
        typ = f["type"]
        if typ == "text":
            cols.append(f"  {q(col)} TEXT")
        elif typ == "int":
            cols.append(f"  {q(col)} BIGINT")
        elif typ == "date":
            cols.append(f"  {q(col)} DATE")
        elif typ == "fk":
            cols.append(f"  {q(col)} BIGINT")
            fk_constraints.append(
                f'  FOREIGN KEY ({q(col)}) REFERENCES {q(schema)}.{q(f["ref_table"])}({q(f["ref_pk"])})'
            )
        else:
            cols.append(f"  {q(col)} TEXT")
    if fk_constraints:
        cols.extend(fk_constraints)
    return f"CREATE TABLE {q(schema)}.{q(name)} (\n" + ",\n".join(cols) + "\n);"

def build_junction_table(schema: str, name: str, left: str, left_pk: str, right: str, right_pk: str) -> str:
    """Build CREATE TABLE SQL for a junction table (composite PK + FKs)."""
    return f"""
CREATE TABLE {q(schema)}.{q(name)} (
  {q(left + "_id")} BIGINT NOT NULL,
  {q(right + "_id")} BIGINT NOT NULL,
  PRIMARY KEY ({q(left + "_id")}, {q(right + "_id")}),
  FOREIGN KEY ({q(left + "_id")}) REFERENCES {q(schema)}.{q(left)}({q(left_pk)}),
  FOREIGN KEY ({q(right + "_id")}) REFERENCES {q(schema)}.{q(right)}({q(right_pk)})
);
""".strip()

def build_ui_meta_table(schema: str) -> str:
    """Build CREATE TABLE SQL for the UI metadata table (_tool_table_meta)."""
    return f"""
CREATE TABLE IF NOT EXISTS {q(schema)}.{q("_tool_table_meta")} (
  table_name TEXT PRIMARY KEY,
  table_type TEXT NOT NULL CHECK (table_type IN ('entity','lookup','junction')),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
""".strip()

def build_ui_meta_seed(schema: str, tables: Dict[str, Any]) -> List[str]:
    """Build INSERTs for UI meta, one row per table."""
    out: List[str] = []
    for t in tables.values():
        out.append(
            f"INSERT INTO {q(schema)}.{q('_tool_table_meta')} (table_name, table_type) "
            f"VALUES ({sql_quote_text(t['name'])}, {sql_quote_text(t['type'])}) "
            f"ON CONFLICT (table_name) DO UPDATE SET table_type = EXCLUDED.table_type, updated_at = now();"
        )
    return out

# -------------------- SQL LITERALS --------------------

def sql_literal(value: str, field: Dict[str, Any], schema: str, date_formats: List[str]) -> str:
    """Convert a CSV value into SQL literal text (including FK subselects)."""
    typ = field["type"]
    v = (value or "").strip()
    if typ == "text":
        return "NULL" if not v else sql_quote_text(v)
    if typ == "int":
        if not v:
            return "NULL"
        try:
            return str(int(v))
        except ValueError:
            return "NULL"
    if typ == "date":
        iso = parse_date_to_iso(v, date_formats)
        return "NULL" if not iso else f"DATE {sql_quote_text(iso)}"
    if typ == "fk":
        # lookup by name (lookups are inserted earlier)
        if not v:
            return "NULL"
        ref_table = field["ref_table"]
        return f"(SELECT {q('id')} FROM {q(schema)}.{q(ref_table)} WHERE name = {sql_quote_text(v)})"
    return "NULL" if not v else sql_quote_text(v)

# -------------------- LOOKUP EXTRACTION --------------------

def collect_lookup_sources(job_tables: List[Dict[str, Any]]) -> Dict[str, Set[str]]:
    """
    Return map: lookup_table_name -> set(csv_column_names it depends on)
    based on entity FK fields and junction left/right csv mappings.
    """
    sources: Dict[str, Set[str]] = {}
    for t in job_tables:
        if t["table_type"] == "entity":
            for f in t.get("fields", []):
                if f.get("type") == "fk" and f.get("ref") and f.get("csv"):
                    sources.setdefault(normalize(f["ref"]), set()).add(f["csv"])
        if t["table_type"] == "junction":
            # junctions can reference lookup tables on either side
            if "left_csv" in t and t.get("left"):
                sources.setdefault(normalize(t["left"]), set()).add(t["left_csv"])
            if "right_csv" in t and t.get("right"):
                sources.setdefault(normalize(t["right"]), set()).add(t["right_csv"])
    return sources

def extract_lookup_values(
    rows: List[Dict[str, str]],
    lookup_sources: Dict[str, Set[str]],
    junction_specs: List[Dict[str, Any]],
) -> Dict[str, Set[str]]:
    """Extract distinct lookup values per lookup table name."""
    # per-junction split spec overrides
    split_by_csv_col: Dict[str, str] = {}
    for j in junction_specs:
        split_spec = j.get("split", "\\n")
        if j.get("left_csv"):
            split_by_csv_col[j["left_csv"]] = split_spec
        if j.get("right_csv"):
            split_by_csv_col[j["right_csv"]] = split_spec

    values: Dict[str, Set[str]] = {tbl: set() for tbl in lookup_sources.keys()}
    for r in rows:
        for tbl, cols in lookup_sources.items():
            for col in cols:
                raw = r.get(col, "")
                if col in split_by_csv_col:
                    parts = split_values(raw, split_by_csv_col[col])
                else:
                    parts = [str(raw).strip()] if str(raw).strip() else []
                for p in parts:
                    if p:
                        values[tbl].add(p)
    return values

def build_lookup_inserts(schema: str, tables: Dict[str, Any], lookup_values: Dict[str, Set[str]]) -> Tuple[List[str], Dict[str, int]]:
    """Build INSERT statements for all lookup tables (distinct values)."""
    sql: List[str] = []
    counts: Dict[str, int] = {}
    for tbl, vals in lookup_values.items():
        if not vals:
            counts[tbl] = 0
            continue
        for v in sorted(vals, key=lambda x: x.lower()):
            sql.append(
                f"INSERT INTO {q(schema)}.{q(tbl)} (name) VALUES ({sql_quote_text(v)}) "
                f"ON CONFLICT (name) DO NOTHING;"
            )
        counts[tbl] = len(vals)
    return sql, counts

# -------------------- JUNCTION INSERTS --------------------

def pair_values(left_vals: List[str], right_vals: List[str], mode: str) -> Iterable[Tuple[str, str]]:
    """Pair left/right values either by zip (positional) or cartesian."""
    if mode == "zip":
        for a, b in zip(left_vals, right_vals):
            if a and b:
                yield a, b
    else:
        for a in left_vals:
            for b in right_vals:
                if a and b:
                    yield a, b

def build_junction_inserts(
    schema: str,
    junction: Dict[str, Any],
    tables: Dict[str, Any],
    rows: List[Dict[str, str]],
) -> Tuple[List[str], int, int]:
    """
    Build INSERT statements for a junction table.
    Returns: (sql_lines, row_pairs_emitted, warnings)
    """
    name = junction["name"]
    left = normalize(junction["left"])
    right = normalize(junction["right"])
    left_id_col = f"{left}_id"
    right_id_col = f"{right}_id"
    split_spec = junction.get("split", "\\n")
    pairing = junction.get("pairing", "cartesian")
    emitted = 0
    warnings = 0
    out: List[str] = []
    for r in rows:
        csv_id = get_csv_id(r)
        left_csv = junction.get("left_csv")
        right_csv = junction.get("right_csv")
        # Determine left ids/values
        if tables[left]["type"] == "entity":
            # left id comes from entity ID (prefer CSV ID for determinism)
            if csv_id is None:
                warnings += 1
                continue
            left_id_exprs = [str(csv_id)]
        else:
            if not left_csv:
                warnings += 1
                continue
            left_vals = split_values(r.get(left_csv, ""), split_spec)
            left_id_exprs = [
                f"(SELECT {q('id')} FROM {q(schema)}.{q(left)} WHERE name = {sql_quote_text(v)})"
                for v in left_vals
            ]
        # Determine right ids/values (junctions in this tool always map to lookup on right, or lookup via right_csv)
        if tables[right]["type"] == "entity":
            if csv_id is None:
                warnings += 1
                continue
            right_id_exprs = [str(csv_id)]
        else:
            if not right_csv:
                warnings += 1
                continue
            right_vals = split_values(r.get(right_csv, ""), split_spec)
            right_id_exprs = [
                f"(SELECT {q('id')} FROM {q(schema)}.{q(right)} WHERE name = {sql_quote_text(v)})"
                for v in right_vals
            ]
        # Pairing rules: if both sides are lookups and both have csv lists, we can pair by zip/cartesian
        if tables[left]["type"] != "entity" and tables[right]["type"] != "entity" and left_csv and right_csv:
            left_vals = split_values(r.get(left_csv, ""), split_spec)
            right_vals = split_values(r.get(right_csv, ""), split_spec)
            if pairing == "zip" and len(left_vals) != len(right_vals) and left_vals and right_vals:
                warnings += 1  # mismatch, but still zip what we can
            for lv, rv in pair_values(left_vals, right_vals, pairing):
                out.append(
                    f"INSERT INTO {q(schema)}.{q(name)} ({q(left_id_col)}, {q(right_id_col)}) VALUES ("
                    f"(SELECT {q('id')} FROM {q(schema)}.{q(left)} WHERE name = {sql_quote_text(lv)}), "
                    f"(SELECT {q('id')} FROM {q(schema)}.{q(right)} WHERE name = {sql_quote_text(rv)})"
                    f") ON CONFLICT DO NOTHING;"
                )
                emitted += 1
            continue

        # Otherwise: join each left id to each right id (entity<->lookup cases)
        for le in left_id_exprs:
            for re_ in right_id_exprs:
                out.append(
                    f"INSERT INTO {q(schema)}.{q(name)} ({q(left_id_col)}, {q(right_id_col)}) "
                    f"VALUES ({le}, {re_}) ON CONFLICT DO NOTHING;"
                )
                emitted += 1
    return out, emitted, warnings

# -------------------- ENTITY INSERTS --------------------

def build_entity_inserts(
    schema: str,
    entity: Dict[str, Any],
    rows: List[Dict[str, str]],
    date_formats: List[str],
) -> Tuple[List[str], int, int]:
    """Build INSERTs for entity rows. Uses CSV ID if present to make ids deterministic."""
    out: List[str] = []
    inserted = 0
    warnings = 0
    for r in rows:
        cols: List[str] = []
        vals: List[str] = []
        csv_id = get_csv_id(r)
        if csv_id is not None and entity["pk"] == "id":
            cols.append(q(entity["pk"]))
            vals.append(str(csv_id))
        for f in entity["fields"]:
            cols.append(q(f["col"]))
            vals.append(sql_literal(r.get(f["csv"], ""), f, schema, date_formats))
        out.append(
            f"INSERT INTO {q(schema)}.{q(entity['name'])} "
            f"({', '.join(cols)}) VALUES ({', '.join(vals)});"
        )
        inserted += 1
        # basic guard: if we're expecting ID and don't have it, warn
        if entity["pk"] == "id" and csv_id is None:
            warnings += 1
    return out, inserted, warnings

# -------------------- SUMMARY --------------------

def print_summary(
    job_name: str,
    rows_read: int,
    tables: Dict[str, Any],
    seed_ui_meta: bool,
    lookup_value_counts: Dict[str, int],
    entity_inserts: int,
    entity_id_warnings: int,
    junction_counts: Dict[str, int],
    junction_warnings: Dict[str, int],
) -> None:
    """Print a compact per-job summary."""
    lookups = [t for t in tables.values() if t["type"] == "lookup"]
    entities = [t for t in tables.values() if t["type"] == "entity"]
    junctions = [t for t in tables.values() if t["type"] == "junction"]
    print("\n" + "=" * 70)
    print(f"JOB: {job_name}")
    print(f"CSV rows read: {rows_read}")
    print(f"Seed UI meta: {seed_ui_meta}")
    print("-" * 70)
    print(f"Tables: {len(tables)}  (entity={len(entities)}, lookup={len(lookups)}, junction={len(junctions)})")
    if lookups:
        print("\nLookup values extracted:")
        for t in sorted(lookups, key=lambda x: x["name"]):
            c = lookup_value_counts.get(t["name"], 0)
            print(f"  - {t['name']}: {c}")
    if entities:
        e = entities[0]
        print("\nEntity inserts:")
        print(f"  - {e['name']}: {entity_inserts} rows")
        if entity_id_warnings:
            print(f"  ! Warning: {entity_id_warnings} rows had no usable CSV ID (ids may not be deterministic)")
    if junctions:
        print("\nJunction inserts emitted:")
        for t in sorted(junctions, key=lambda x: x["name"]):
            c = junction_counts.get(t["name"], 0)
            w = junction_warnings.get(t["name"], 0)
            extra = f" (warnings={w})" if w else ""
            print(f"  - {t['name']}: {c}{extra}")
    print("=" * 70 + "\n")

# -------------------- MAIN --------------------

def main() -> None:
    """Generate SQL for every job in the config."""
    cfg = json.load(open(CONFIG_PATH, "r", encoding="utf-8"))
    for job_name, job in cfg["jobs"].items():
        schema = "public"
        csv_cfg = job["csv"]
        output = job["output"]
        seed_ui_meta = bool(output.get("seed_ui_meta"))
        csv_path = Path(csv_cfg["path"])
        rows = list(csv.DictReader(
            open(csv_path, encoding=csv_cfg.get("encoding", "utf-8")),
            delimiter=csv_cfg.get("delimiter", ",")
        ))
        date_formats = csv_cfg.get("date_formats", ["%Y-%m-%d"])
        sql: List[str] = []
        sql.append(f"-- generated for job: {job_name}")
        db_name = output.get("database_name")
        if db_name:
            # NOTE: Do NOT include semicolons in these comment helpers.
            # The backend importer splits on ';' without comment awareness;
            # a semicolon inside a comment gets treated as a separator and
            # can produce a "comment-only" statement, which Postgres rejects
            # with: "can't execute an empty query".
            sql.append(f'-- psql helper (optional): CREATE DATABASE "{db_name}"')
            sql.append(f'-- psql helper (optional): \\c {db_name}')
        sql.append(f"SET search_path TO {q(schema)};")
        # -------- RESOLVE TABLE METADATA --------
        tables: Dict[str, Any] = {}
        for t in job["tables"]:
            name = normalize(t["name"])
            tables[name] = {
                "name": name,
                "pk": normalize(t.get("id_column", "id")),
                "type": t["table_type"],
                "raw": t
            }
        # -------- UI META --------
        if seed_ui_meta:
            sql.append(build_ui_meta_table(schema))
        # -------- CREATE TABLES (lookups -> entity -> junctions) --------
        for t in tables.values():
            if t["type"] == "lookup":
                sql.append(build_lookup_table(schema, t["name"], t["pk"]))
        for t in tables.values():
            if t["type"] == "entity":
                fields = []
                for f in t["raw"]["fields"]:
                    field = {"col": normalize(f["name"]), "type": f["type"], "csv": f["csv"]}
                    if f["type"] == "fk":
                        ref = normalize(f["ref"])
                        field["ref_table"] = ref
                        field["ref_pk"] = tables[ref]["pk"]
                    fields.append(field)
                t["fields"] = fields
                sql.append(build_entity_table(schema, t["name"], t["pk"], fields))
        for t in tables.values():
            if t["type"] == "junction":
                raw = t["raw"]
                left = normalize(raw["left"])
                right = normalize(raw["right"])
                # store normalized left/right so later code can use them reliably
                t["left"] = left
                t["right"] = right
                t["left_csv"] = raw.get("left_csv")
                t["right_csv"] = raw.get("right_csv")
                t["split"] = raw.get("split", "\\n")
                t["pairing"] = raw.get("pairing", "cartesian")
                sql.append(build_junction_table(schema, t["name"], left, tables[left]["pk"], right, tables[right]["pk"]))
        # -------- UI META SEED --------
        if seed_ui_meta:
            sql.extend(build_ui_meta_seed(schema, tables))
        # -------- LOOKUP INSERTS --------
        lookup_sources = collect_lookup_sources(job["tables"])
        junction_specs = [t for t in job["tables"] if t["table_type"] == "junction"]
        lookup_values = extract_lookup_values(rows, lookup_sources, junction_specs)
        lookup_sql, lookup_counts = build_lookup_inserts(schema, tables, lookup_values)
        sql.extend(lookup_sql)
        # -------- ENTITY INSERTS --------
        entity = next(t for t in tables.values() if t["type"] == "entity")
        entity_sql, entity_insert_count, entity_id_warnings = build_entity_inserts(schema, entity, rows, date_formats)
        sql.extend(entity_sql)
        # -------- JUNCTION INSERTS --------
        junction_counts: Dict[str, int] = {}
        junction_warnings: Dict[str, int] = {}
        for t in tables.values():
            if t["type"] != "junction":
                continue
            jsql, emitted, warns = build_junction_inserts(schema, t, tables, rows)
            sql.extend(jsql)
            junction_counts[t["name"]] = emitted
            junction_warnings[t["name"]] = warns
        # -------- WRITE SQL --------
        out = Path(output["sql_output_path"])
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text("\n".join(sql) + "\n", encoding="utf-8")
        print_summary(
            job_name=job_name,
            rows_read=len(rows),
            tables=tables,
            seed_ui_meta=seed_ui_meta,
            lookup_value_counts=lookup_counts,
            entity_inserts=entity_insert_count,
            entity_id_warnings=entity_id_warnings,
            junction_counts=junction_counts,
            junction_warnings=junction_warnings,
        )
        print(f"[OK] Generated SQL for '{job_name}' -> {out}")

if __name__ == "__main__":
    main()
